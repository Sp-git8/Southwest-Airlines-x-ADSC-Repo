# -*- coding: utf-8 -*-
"""Model_Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JojYPsDOAQQvtQoMe8-l9D1YP5y5c6Dy
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/MyDrive/Weather Data Project (Southwest Airlines)/Matthew/final_dataset.csv')

import pandas as pd

# Suppose df is your original DataFrame
group_cols = ['FlightDate', 'Origin_Airport']  # use correct date and airport columns

# For each weather feature, fill NaN with group mean for that day+airport
weather_cols = [
    'temperature_C', 'dew_point_C', 'humidity_percent', 'precipitation_mm',
    'wind_direction_deg', 'wind_speed_kmh', 'pressure_hPa',
    'snow_mm'
]

for col in weather_cols:
    df[col] = df.groupby(group_cols)[col].transform(lambda x: x.fillna(x.mean()))

# If any NaNs remain (possibly if an entire group is missing), use a global mean as fallback
df[weather_cols] = df[weather_cols].fillna(df[weather_cols].mean())

print(df.isnull().sum())

df = df.drop(columns=['wind_gust_kmh','weather_code', 'CancellationCode'])

"""## HistGradient"""

def compute_disruption(row):

    if pd.isna(row['CancelledWeather']) or pd.isna(row['WeatherDelay']):
        return 0
    elif row['CancelledWeather'] == 1:
        return 100
    elif row['WeatherDelay'] > 30:
        return 75
    elif row['WeatherDelay'] > 0:
        return 25
    else:
        return 0

df['disruption_score'] = df.apply(compute_disruption, axis=1)

import pandas as pd
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler


features = [
    'temperature_C', 'dew_point_C', 'humidity_percent', 'precipitation_mm',
    'wind_direction_deg', 'wind_speed_kmh', 'pressure_hPa',
    'snow_mm'
]
X = df[features]
y = df['disruption_score']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = HistGradientBoostingRegressor()
model.fit(X_train, y_train)


y_pred = model.predict(X_test)

scaler = MinMaxScaler(feature_range=(0, 100))
y_pred_scaled = scaler.fit_transform(y_pred.reshape(-1, 1)).flatten()


result = X_test.copy()
result['predicted_weather_score'] = y_pred_scaled
print(result[['predicted_weather_score']].head())

import matplotlib.pyplot as plt


plt.hist(result['predicted_weather_score'], bins=30, color='skyblue', edgecolor='black')
plt.xlabel('Predicted Weather Disruption Score')
plt.ylabel('Frequency')
plt.title('Distribution of Weather Disruption Scores')
plt.show()

"""## Neural Network"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import EarlyStopping

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

model = Sequential([
    Dense(64, input_dim=X_scaled.shape[1], activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mse')

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model.fit(X_scaled, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping])


weather_score = model.predict(X_scaled)
weather_score = MinMaxScaler((0, 100)).fit_transform(weather_score).flatten()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R²: {r2:.3f}")

"""## Light GBM

"""

import pandas as pd
import numpy as np
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import shap
from lightgbm import early_stopping, log_evaluation

# ------------------------
# Preprocess temporal features
# ------------------------
df['FlightDate'] = pd.to_datetime(df['FlightDate'])
df['hour'] = df['CRSDepTime'] // 100
df['day_of_week'] = df['FlightDate'].dt.weekday
df['month'] = df['FlightDate'].dt.month


feature_cols = ['temperature_C','dew_point_C','humidity_percent',
                'precipitation_mm','snow_mm','wind_speed_kmh','pressure_hPa',
                'hour','day_of_week','month', 'wind_direction_deg']
X = df[feature_cols]
y = df['WeatherDelay']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LGBMRegressor(objective="quantile", alpha=0.9)


model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='rmse',
    callbacks=[early_stopping(stopping_rounds=50), log_evaluation(50)]
)


from sklearn.metrics import mean_squared_error
import numpy as np

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.2f}")

# 1) SHAP feature importances (global)
df_sample = (
    df.groupby("Origin_Airport")
      .apply(lambda g: g.sample(min(len(g), 200000), random_state=42))
      .reset_index(drop=True)
)

explainer = shap.Explainer(model)
shap_values = explainer(df_sample[feature_cols])

# Mean absolute SHAP per feature (importance)
global_importance = np.abs(shap_values.values).mean(axis=0)
importance_weights = pd.Series(global_importance, index=feature_cols)
importance_weights = importance_weights / importance_weights.sum()   # normalize to sum=1
importance_weights

weather_cols = [c for c in weather_cols if c in df.columns]

airport_means = df.groupby("Origin_Airport")[weather_cols].mean()
airport_norm = (airport_means - airport_means.min()) / (airport_means.max() - airport_means.min())
airport_norm = airport_norm.fillna(0)
# use only weather feature weights
weather_weights = importance_weights[weather_cols]

# weighted severity score
airport_means["weather_score"] = (airport_norm * weather_weights).sum(axis=1)

# scale 0–100
airport_means["weather_score"] = 100 * airport_means["weather_score"] / airport_means["weather_score"].max()

final_scores = airport_means["weather_score"].sort_values(ascending=False)
import json

final_scores_dict = final_scores.to_dict()

output_path = '/content/drive/My Drive/weather_scores.json'
with open(output_path, 'w') as f:
    json.dump(final_scores_dict, f)

print(f"Weather scores saved to {output_path}")
print(final_scores)

import pandas as pd
import numpy as np

# Use only weather columns
airport_data = airport_means[weather_cols].copy()

# Normalized values
airport_data_norm = (airport_data - airport_data.min()) / (airport_data.max() - airport_data.min())
airport_data_norm.columns = [c + "_norm" for c in airport_data_norm.columns]

# SHAP feature weights
weather_weights = importance_weights[weather_cols]
weather_weights.name = "shap_weight"

# Merge into one dataset
plot_df = pd.concat([airport_data, airport_data_norm], axis=1)
plot_df["weather_score"] = airport_means["weather_score"]

plot_df

plot_df.to_csv("Scores.csv")

import seaborn as sns
import matplotlib.pyplot as plt

def plot_feature_kde(feature):
    plt.figure(figsize=(10,6))

    # KDE of ALL airport values
    sns.kdeplot(
        data=plot_df,
        x=feature,
        fill=True,
        alpha=0.4,
        linewidth=2
    )

    # Show each airport as a vertical point
    for airport, value in plot_df[feature].items():
        plt.scatter(value, 0, s=80, label=airport)

    plt.title(f"KDE Distribution of {feature} Across Major Airports")
    plt.xlabel(feature)
    plt.ylabel("Density")
    plt.grid(True)
    plt.show()

import ipywidgets as widgets
from ipywidgets import interact

@interact(feature=weather_cols)
def interactive_kde(feature):
    plot_feature_kde(feature)

import pandas as pd

# Example airport lat/lon for your 19 airports
airport_coords = pd.DataFrame({
    "Origin_Airport": ['ANC','ATL','BOS','CLT','DEN','HOU','IAD','IAH','JFK',
                       'LAS','LAX','LGA','MCO','MIA','ORD','PDX','PHX','SEA','SFO'],
    "lat": [61.1744,33.6407,42.3656,35.2140,39.8617,29.9902,38.9445,29.9844,40.6413,
            36.0801,33.9416,40.7769,28.4312,25.7959,41.9742,45.5887,33.4342,47.4489,37.6213],
    "lon": [-149.996, -84.4277, -71.0096, -80.9431, -104.673, -95.3368, -77.4564, -95.3414, -73.7781,
            -115.152, -118.4085, -73.8740, -81.3081, -80.2870, -87.9073, -122.5972, -112.0099, -122.3093, -122.3790]
})

# Merge with weather scores
airport_map_df = airport_coords.merge(
    airport_means[["weather_score"]].reset_index(),
    on="Origin_Airport"
)

# Optional: include normalized features for filtering later
for f in weather_cols:
    airport_map_df[f] = airport_means[f]

import plotly.express as px

fig = px.scatter_mapbox(
    airport_map_df,
    lat="lat",
    lon="lon",
    hover_name="Origin_Airport",
    hover_data=["weather_score"] + weather_cols,  # shows values on hover
    color="weather_score",
    color_continuous_scale="YlOrRd",
    size="weather_score",  # optional: bigger circle = higher impact
    size_max=25,
    zoom=3,
    mapbox_style="carto-positron"  # clean background
)

fig.update_layout(
    title="Airport Weather Impact Map",
    margin={"r":0,"t":40,"l":0,"b":0}
)

fig.show()

